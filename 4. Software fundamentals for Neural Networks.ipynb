{"cells":[{"cell_type":"markdown","metadata":{"id":"vgFXLV7689tz","cell_id":"6d9c589221ed4545bb09d4dcbd59fb40","deepnote_cell_type":"markdown"},"source":["# Software fundamentals for Neural Networks\n","February 2024\n","* Johanna Norell\n","* Andreas Helgesson\n","* Axel Blom"]},{"cell_type":"code","metadata":{"id":"5f0fea34","source_hash":null,"execution_start":1708942039778,"execution_millis":474,"deepnote_to_be_reexecuted":false,"cell_id":"1836608ff32b4223a960c3a1586da0d3","deepnote_cell_type":"code","outputId":"04614113-f8ae-4374-fa2d-e8d374cdfd9d"},"source":["# Import necessary packages\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch import nn"],"execution_count":null,"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"56578f6f","cell_id":"3ed4970730dd47389756c3d51bd4aad5","deepnote_cell_type":"markdown"},"source":["# Extracting the Data"]},{"cell_type":"code","metadata":{"id":"0bOXEcNYiW-A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b75da81b-94fa-49c5-ed8b-ab20c6399a86","source_hash":null,"execution_start":1708942040254,"execution_millis":1243,"deepnote_to_be_reexecuted":false,"cell_id":"f5c2b0956ac64dd9a8e9cc1e1a6b9c00","deepnote_cell_type":"code"},"source":["!wget --no-check-certificate https://www.cse.chalmers.se/~richajo/dit866/assignments/a4/data/a4_synthetic.csv"],"execution_count":null,"outputs":[{"name":"stdout","text":"--2024-02-26 10:07:20--  https://www.cse.chalmers.se/~richajo/dit866/assignments/a4/data/a4_synthetic.csv\nResolving www.cse.chalmers.se (www.cse.chalmers.se)... 129.16.222.93\nConnecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.222.93|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5874 (5.7K) [text/csv]\nSaving to: ‘a4_synthetic.csv.35’\n\na4_synthetic.csv.35 100%[===================>]   5.74K  --.-KB/s    in 0s      \n\n2024-02-26 10:07:21 (79.3 MB/s) - ‘a4_synthetic.csv.35’ saved [5874/5874]\n\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"1761f4d2","cell_id":"cd7109f119af4a0f8e4d9a4b57167f0b","deepnote_cell_type":"markdown"},"source":["Loading the synthetic dataset."]},{"cell_type":"code","metadata":{"id":"aacba01e","source_hash":null,"execution_start":1708942041511,"execution_millis":176,"deepnote_to_be_reexecuted":false,"cell_id":"5ae5eab4b8344fd8a9fbe40bd11d4f80","deepnote_cell_type":"code"},"source":["# You may need to edit the path, depending on where you put the files.\n","data = pd.read_csv('a4_synthetic.csv')\n","\n","X = data.drop(columns='y').to_numpy()\n","Y = data.y.to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b166d6ba","cell_id":"fcc05b9250e24e93b202bd2b498ff0d4","deepnote_cell_type":"markdown"},"source":["Training a linear regression model for this synthetic dataset."]},{"cell_type":"code","metadata":{"id":"111ff34b","colab":{"base_uri":"https://localhost:8080/"},"outputId":"68efd45b-cd6d-44dd-9a77-26613acb985d","source_hash":null,"execution_start":1708942041581,"execution_millis":202,"deepnote_to_be_reexecuted":false,"cell_id":"317d951084564a858108c61b4e0ed28f","deepnote_cell_type":"code"},"source":["np.random.seed(1)\n","\n","w_init = np.random.normal(size=(2, 1))\n","b_init = np.random.normal(size=(1, 1))\n","\n","# Declaring tensors with parameters\n","w = torch.tensor(w_init, dtype=torch.float, requires_grad=True)\n","b = torch.tensor(b_init, dtype=torch.float, requires_grad=True)\n","\n","#Declaring learning rate and algorithm used for optimization\n","eta = 1e-2\n","opt = torch.optim.SGD([w,b], lr=eta)\n","\n","#Training the model\n","for i in range(10):\n","\n","    sum_err = 0\n","\n","    for row in range(X.shape[0]):\n","        x = torch.tensor(X[[row], :], dtype=torch.float32)\n","        y = torch.tensor(Y[[row]])\n","\n","        # Forward pass.\n","        y_pred = x@w + b # linear: y=ax+b\n","        err = (y - y_pred)**2\n","\n","        # Backward and update.\n","        opt.zero_grad() # Resets the gradients of all optimized\n","        err.backward() # Computes the gradient of current tensor wrt graph leaves\n","        opt.step() # Performs a single optimization step (parameter update)\n","\n","        # For statistics.\n","        sum_err += err.item()\n","\n","    mse = sum_err / X.shape[0]\n","    print(f'Epoch {i+1}: MSE =', mse)"],"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1: MSE = 0.7999662623416189\nEpoch 2: MSE = 0.01739239492736053\nEpoch 3: MSE = 0.009377417782394874\nEpoch 4: MSE = 0.009355326801683737\nEpoch 5: MSE = 0.009365440374032813\nEpoch 6: MSE = 0.0093669889321558\nEpoch 7: MSE = 0.009367207355115736\nEpoch 8: MSE = 0.009367237678977408\nEpoch 9: MSE = 0.00936724428618897\nEpoch 10: MSE = 0.009367244374616222\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"ee3f221d","cell_id":"88dbdc4f048f44e69eaa6c7c54a2f771","deepnote_cell_type":"markdown"},"source":["# Building Tensors"]},{"cell_type":"code","metadata":{"id":"56be71d4","source_hash":null,"execution_start":1708942041785,"execution_millis":298,"deepnote_to_be_reexecuted":false,"cell_id":"2c328a550075446bb687703fabe4c4d5","deepnote_cell_type":"code"},"source":["class Tensor:\n","\n","    # Constructor. Just store the input values.\n","    def __init__(self, data, requires_grad=False, grad_fn=None):\n","        self.data = data\n","        self.shape = data.shape\n","        self.grad_fn = grad_fn\n","        self.requires_grad = requires_grad\n","        self.grad = None\n","\n","    # So that we can print the object or show it in a notebook cell.\n","    def __repr__(self):\n","        dstr = repr(self.data)\n","        if self.requires_grad:\n","            gstr = ', requires_grad=True'\n","        elif self.grad_fn is not None:\n","            gstr = f', grad_fn={self.grad_fn}'\n","        else:\n","            gstr = ''\n","        return f'Tensor({dstr}{gstr})'\n","\n","    # Extract one numerical value from this tensor.\n","    def item(self):\n","        return self.data.item()\n","\n","\n","    # Operator +\n","    def __add__(self, right):\n","        return addition(self, right)\n","\n","    # Operator -\n","    def __sub__(self, right):\n","        return subtraction(self, right)\n","\n","    # Operator @\n","    def __matmul__(self, right):\n","        return multiplication(self, right)\n","\n","    # Operator **\n","    def __pow__(self, right):\n","        # NOTE! We are assuming that right is an integer here, not a Tensor!\n","        if not isinstance(right, int):\n","            raise Exception('only integers allowed')\n","        if right < 2:\n","            raise Exception('power must be >= 2')\n","        return power(self,right)\n","\n","\n","    # Backward computations. Will be implemented in Task 4.\n","    def backward(self, grad_output=None):\n","        # We first check if this tensor has a grad_fn: that is, one of the\n","        # nodes that you defined in Task 3.\n","        if self.grad_fn is not None: # If has been created by a computation\n","            # If grad_fn is defined, we have computed this tensor using some operation.\n","            if grad_output is None:\n","                # This is the starting point of the backward computation.\n","                # This will typically be the tensor storing the output of\n","                # the loss function, on which we have called .backward()\n","                # in the training loop.\n","\n","                # TODO compute grad output\n","                grad_output = np.array([1])\n","                self.grad_fn.backward(grad_output)\n","\n","            else:\n","                # This is an intermediate node in the computational graph.\n","                # This corresponds to any intermediate computation, such as\n","                # a hidden layer.\n","\n","                # TODO compute(update) grad output\n","                self.grad_fn.backward(grad_output)\n","        else:\n","            # If grad_fn is not defined, this is an endpoint in the computational\n","            # graph: learnable model parameters or input data.\n","            if self.requires_grad:\n","                # This tensor *requires* a gradient to be computed. This will\n","                # typically be a tensor that holds learnable parameters.\n","\n","                # TODO compute(update) grad output\n","                self.grad = np.transpose(grad_output)\n","                return # Terminate recursion\n","            else:\n","                # This tensor *does not require* a gradient to be computed. This\n","                # will typically be a tensor holding input data.\n","                #self.grad = grad_output\n","                return # Terminate recursion\n","\n","\n","# A small utility where we simply create a Tensor object. We use this to\n","# mimic torch.tensor.\n","def tensor(data, requires_grad=False):\n","    return Tensor(data, requires_grad)\n","\n","\n","# Helper functions to implement the various arithmetic operations.\n","# Takes two tensors as input, and returns a new tensor holding\n","# the result of an element-wise addition on the two input tensors.\n","\n","#Helper method for addition\n","def addition(left, right):\n","    new_data = left.data + right.data\n","    if left.requires_grad or right.requires_grad:\n","      grad_fn = AdditionNode(left, right)\n","    else:\n","      grad_fn = None\n","    return Tensor(new_data,left.requires_grad or right.requires_grad, grad_fn=grad_fn)\n","\n","#Helper method for subtraction\n","def subtraction(left, right):\n","  new_data = left.data - right.data\n","  if left.requires_grad or right.requires_grad:\n","      grad_fn = SubtractionNode(left, right)\n","  else:\n","      grad_fn = None\n","  return Tensor(new_data,left.requires_grad or right.requires_grad, grad_fn=grad_fn)\n","\n","#Helper method for multiplication\n","def multiplication(left, right):\n","  new_data = left.data @ right.data\n","  if left.requires_grad or right.requires_grad:\n","      grad_fn = MultiplicationNode(left, right)\n","  else:\n","      grad_fn = None\n","  return Tensor(new_data,left.requires_grad or right.requires_grad, grad_fn=grad_fn)\n","\n","#Helper method for power\n","def power(left, right):\n","  new_data = left.data ** right\n","  if left.requires_grad:\n","      grad_fn = PowerNode(left, right)\n","  else:\n","      grad_fn = None\n","  return Tensor(new_data,left.requires_grad, grad_fn=grad_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36d0f04c","cell_id":"f907f3ff9fab4d2f8288e98cb039c25f","deepnote_cell_type":"markdown"},"source":["Some sanity checks."]},{"cell_type":"code","metadata":{"id":"f2014827","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fcfea363-74d2-417d-be98-96f82fdea8d7","source_hash":null,"execution_start":1708942041796,"execution_millis":287,"deepnote_to_be_reexecuted":false,"cell_id":"b8c693f9fcdf474d8b3bd432a681e726","deepnote_cell_type":"code"},"source":["# Two tensors holding row vectors.\n","x1 = tensor(np.array([[2.0, 3.0]]))\n","x2 = tensor(np.array([[1.0, 4.0]]))\n","# A tensors holding a column vector.\n","w = tensor(np.array([[-1.0], [1.2]]))\n","\n","# Test the arithmetic operations.\n","test_plus = x1 + x2\n","test_minus = x1 - x2\n","test_power = x2 ** 2\n","test_matmul = x1 @ w\n","\n","print(f'Test of addition: {x1.data} + {x2.data} = {test_plus.data}')\n","print(f'Test of subtraction: {x1.data} - {x2.data} = {test_minus.data}')\n","print(f'Test of power: {x2.data} ** 2 = {test_power.data}')\n","print(f'Test of matrix multiplication: {x1.data} @ {w.data} = {test_matmul.data}')\n","\n","# Check that the results are as expected. Will crash if there is a miscalculation.\n","assert(np.allclose(test_plus.data, np.array([[3.0, 7.0]])))\n","assert(np.allclose(test_minus.data, np.array([[1.0, -1.0]])))\n","assert(np.allclose(test_power.data, np.array([[1.0, 16.0]])))\n","assert(np.allclose(test_matmul.data, np.array([[1.6]])))"],"execution_count":null,"outputs":[{"name":"stdout","text":"Test of addition: [[2. 3.]] + [[1. 4.]] = [[3. 7.]]\nTest of subtraction: [[2. 3.]] - [[1. 4.]] = [[ 1. -1.]]\nTest of power: [[1. 4.]] ** 2 = [[ 1. 16.]]\nTest of matrix multiplication: [[2. 3.]] @ [[-1. ]\n [ 1.2]] = [[1.6]]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"7c645c32","cell_id":"2c9166651eca4a10aa0d8afaba9aaf21","deepnote_cell_type":"markdown"},"source":["# Defining node-types"]},{"cell_type":"code","metadata":{"id":"9133db2f","source_hash":null,"execution_start":1708942041811,"execution_millis":272,"deepnote_to_be_reexecuted":false,"cell_id":"f2e968d4cb734138a473156569581d6e","deepnote_cell_type":"code"},"source":["class Node:\n","    def __init__(self):\n","        pass\n","\n","    def backward(self, grad_output):\n","        raise NotImplementedError('Unimplemented')\n","\n","    def __repr__(self):\n","        return str(type(self))\n","\n","# Backward gradient calculations as per:\n","# Addition: g*(left+right) --> g, g\n","# Subtraction: g*(left-right) --> g, -g\n","# Multiplication: g*(left*right) --> g*right, g*left\n","# Power: g*(left^right) --> g*right*left^(right-1), ___\n","\n","#AdditionNode class defined to support backpropagation\n","class AdditionNode(Node):\n","    def __init__(self, left, right):\n","        self.left = left\n","        self.right = right\n","\n","    def backward(self, grad_output):\n","      if self.left.requires_grad:\n","        self.left.backward(grad_output)\n","      if self.right.requires_grad:\n","        self.right.backward(grad_output)\n","\n","#SubtractionNode class defined to support backpropagation\n","class SubtractionNode(Node):\n","    def __init__(self, left, right):\n","        self.left = left\n","        self.right = right\n","\n","    def backward(self, grad_output):\n","      if self.left.requires_grad:\n","        self.left.backward(grad_output)\n","      if self.right.requires_grad:\n","        self.right.backward(-grad_output)\n","\n","#MultiplicationNode class defined to support backpropagation\n","class MultiplicationNode(Node):\n","    def __init__(self, left, right):\n","        self.left = left\n","        self.right = right\n","\n","    def backward(self, grad_output):\n","        if self.left.requires_grad:\n","            self.left.backward(self.right.data * grad_output)\n","        if self.right.requires_grad:\n","            self.right.backward(self.left.data * grad_output)\n","\n","# Adjustment to make node handle matrix multiplication, cannot get it working for task 4/5 but task 6...\n","#    def backward(self, grad_output):\n","#      if self.left.requires_grad:\n","#        if len(self.right.data.shape) > 1:  # Matrix\n","#            grad_left = np.dot(grad_output, self.right.data.T)\n","#        else:\n","#            grad_left = (self.right.data * grad_output)\n","#        self.left.backward(grad_left)\n","#      if self.right.requires_grad:\n","#        if len(self.left.data.shape) > 1:\n","#            grad_right = np.dot(self.left.data.T, grad_output)\n","#        else:\n","#            grad_right = (self.left.data * grad_output)\n","#        self.right.backward(grad_right)\n","\n","#PowerNode class defined to support backpropagation\n","class PowerNode(Node):\n","    def __init__(self, left, right):\n","        self.left = left\n","        self.right = right\n","\n","    def backward(self, grad_output):\n","        if self.left.requires_grad:\n","            self.left.backward(self.right * grad_output * (self.left.data ** (self.right-1))) # Correct /Andreas\n","        # No need to calculate gradient w.r.t. exponent\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9cc1bb77-e869-4e08-8996-3674eed101e6","cell_id":"f3973be4c7af4671bfa8edc348648ec1","deepnote_cell_type":"markdown"},"source":["Sanity check for Task 3."]},{"cell_type":"code","metadata":{"id":"f3276aba-4def-421b-b12e-bf0d7120f19e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9c889c32-e520-4c81-cfdb-6f5afecf815c","source_hash":null,"execution_start":1708942041813,"execution_millis":271,"deepnote_to_be_reexecuted":false,"cell_id":"575ec7a7d0304ab2905464dbc8f3061f","deepnote_cell_type":"code"},"source":["x = tensor(np.array([[2.0, 3.0]]))\n","w1 = tensor(np.array([[1.0, 4.0]]), requires_grad=True)\n","w2 = tensor(np.array([[3.0, -1.0]]), requires_grad=True)\n","\n","test_graph = x + w1 + w2\n","\n","print('Computational graph top node after x + w1 + w2:', test_graph.grad_fn)\n","\n","assert(isinstance(test_graph.grad_fn, AdditionNode))\n","assert(test_graph.grad_fn.right is w2)\n","assert(test_graph.grad_fn.left.grad_fn.left is x)\n","assert(test_graph.grad_fn.left.grad_fn.right is w1)"],"execution_count":null,"outputs":[{"name":"stdout","text":"Computational graph top node after x + w1 + w2: <class '__main__.AdditionNode'>\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"529a9bfb-ea55-4bce-9356-4956316e1904","cell_id":"839429b714414b1a9bf95d4c698353d8","deepnote_cell_type":"markdown"},"source":["Sanity check for Task 4."]},{"cell_type":"code","metadata":{"id":"32687661-a67d-4bef-9a90-7dabb93380a2","source_hash":null,"execution_start":1708942041815,"execution_millis":269,"deepnote_to_be_reexecuted":false,"cell_id":"cb3a278ac232473492be24b4206bc6a1","deepnote_cell_type":"code","outputId":"73ca4ac8-a71f-42b6-ce8d-d7e02fe79e79"},"source":["x = tensor(np.array([[2.0, 3.0]]))\n","w = tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n","y = tensor(np.array([[0.2]]))\n","\n","# We could as well write simply loss = (x @ w - y)**2\n","# We break it down into steps here if you need to debug.\n","\n","model_out = x @ w\n","diff = model_out - y\n","loss = diff ** 2\n","\n","loss.backward()\n","\n","print('Gradient of loss w.r.t. w =\\n', w.grad)\n","\n","assert(np.allclose(w.grad, np.array([[5.6], [8.4]])))\n","assert(x.grad is None)\n","assert(y.grad is None)"],"execution_count":null,"outputs":[{"name":"stdout","text":"Gradient of loss w.r.t. w =\n [[5.6]\n [8.4]]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"105ed8e2","cell_id":"d9e39dc664344a028cd43e7f7bda51ca","deepnote_cell_type":"markdown"},"source":["An equivalent cell using PyTorch code. Your implementation should give the same result for `w.grad`."]},{"cell_type":"code","metadata":{"id":"e72a5687","source_hash":null,"execution_start":1708942041833,"execution_millis":251,"deepnote_to_be_reexecuted":false,"cell_id":"34a8c86182ac4ccca31e9bb4f5c400fa","deepnote_cell_type":"code","outputId":"cbda7292-4abd-4f72-b5a8-584f32f1e915"},"source":["pt_x = torch.tensor(np.array([[2.0, 3.0]]))\n","pt_w = torch.tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n","pt_y = torch.tensor(np.array([[0.2]]))\n","\n","pt_model_out = pt_x @ pt_w\n","pt_model_out.retain_grad() # Keep the gradient of intermediate nodes for debugging.\n","\n","pt_diff = pt_model_out - pt_y\n","pt_diff.retain_grad()\n","\n","pt_loss = pt_diff ** 2\n","pt_loss.retain_grad()\n","\n","pt_loss.backward()\n","pt_w.grad"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"tensor([[5.6000],\n        [8.4000]], dtype=torch.float64)"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"d0b5439b","cell_id":"e6ae3e5bb7d04d3eb2cd3a5313d0b105","deepnote_cell_type":"markdown"},"source":["# Building Optimizers"]},{"cell_type":"code","metadata":{"id":"0b03a8c5","source_hash":null,"execution_start":1708942041869,"execution_millis":394,"deepnote_to_be_reexecuted":false,"cell_id":"a62b99f7dab04e33964b49ae9109a1ca","deepnote_cell_type":"code"},"source":["class Optimizer:\n","    def __init__(self, params):\n","        self.params = params\n","\n","    def zero_grad(self):\n","        for p in self.params:\n","            p.grad = np.zeros_like(p.data)\n","\n","    def step(self):\n","        raise NotImplementedError('Unimplemented')\n","\n","\n","class SGD(Optimizer):\n","    def __init__(self, params, lr):\n","        super().__init__(params)\n","        self.lr = lr\n","\n","    def step(self):\n","        # TODO\n","        for p in self.params:\n","            # Update parameter using SGD: param = param - lr * grad\n","            p.data -= self.lr * p.grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1708942041878,"execution_millis":385,"deepnote_to_be_reexecuted":false,"cell_id":"b6c17ccaef4d4cf2848725f6e8e6d7c8","deepnote_cell_type":"code","id":"TaeSPyh0XjTk","outputId":"bf204d97-cc07-4347-ea3a-f2f73e40abc6"},"source":["np.random.seed(1)\n","\n","w_init = np.random.normal(size=(2, 1))\n","b_init = np.random.normal(size=(1, 1))\n","\n","# We just declare the parameter with our created tensors\n","w = tensor(w_init, requires_grad=True)\n","b = tensor(b_init, requires_grad=True)\n","\n","eta = 1e-2\n","opt = SGD([w,b], lr=eta)\n","\n","for i in range(10):\n","\n","    sum_err = 0\n","\n","    for row in range(X.shape[0]):\n","        x = tensor(X[[row], :])\n","        y = tensor(Y[[row]])\n","\n","        # Forward pass.\n","        y_pred = x @w + b # linear: y=ax+b\n","        err = (y - y_pred)**2\n","\n","        # Backward and update.\n","        opt.zero_grad() # Resets the gradients of all optimized\n","        err.backward() # Computes the gradient of current tensor wrt graph leaves\n","        opt.step() # Performs a single optimization step (parameter update)\n","\n","        # For statistics.\n","        sum_err += err.item()\n","\n","    mse = sum_err / X.shape[0]\n","    print(f'Epoch {i+1}: MSE =', mse)"],"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1: MSE = 0.7999661130823178\nEpoch 2: MSE = 0.017392390107906875\nEpoch 3: MSE = 0.009377418010839892\nEpoch 4: MSE = 0.009355326971438456\nEpoch 5: MSE = 0.009365440968904256\nEpoch 6: MSE = 0.009366989180952533\nEpoch 7: MSE = 0.009367207398577986\nEpoch 8: MSE = 0.009367238983974489\nEpoch 9: MSE = 0.009367243704122532\nEpoch 10: MSE = 0.009367244427185763\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"28bef171","cell_id":"a33d50f85d854faaa0fbd2692c507ea4","deepnote_cell_type":"markdown"},"source":["# Extracting more data"]},{"cell_type":"code","metadata":{"id":"CUWHDZ5Ph5h5","source_hash":null,"execution_start":1708942041974,"execution_millis":1670,"deepnote_to_be_reexecuted":false,"cell_id":"10e89dcf57524a1daf7d347e0ebf093b","deepnote_cell_type":"code","outputId":"3d71cb85-71cf-444a-eb6f-6ccc8011cf20"},"source":["!wget --no-check-certificate https://www.cse.chalmers.se/~richajo/dit866/assignments/a4/data/raisins.csv"],"execution_count":null,"outputs":[{"name":"stdout","text":"--2024-02-26 10:07:22--  https://www.cse.chalmers.se/~richajo/dit866/assignments/a4/data/raisins.csv\nResolving www.cse.chalmers.se (www.cse.chalmers.se)... 129.16.222.93\nConnecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.222.93|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 68230 (67K) [text/csv]\nSaving to: ‘raisins.csv.22’\n\nraisins.csv.22      100%[===================>]  66.63K   315KB/s    in 0.2s    \n\n2024-02-26 10:07:23 (315 KB/s) - ‘raisins.csv.22’ saved [68230/68230]\n\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"id":"da62980a","source_hash":null,"execution_start":1708942043651,"execution_millis":520,"deepnote_to_be_reexecuted":false,"cell_id":"e1e7b3564c134516868285da27944546","deepnote_cell_type":"code"},"source":["from sklearn.preprocessing import scale\n","from sklearn.model_selection import train_test_split\n","\n","# You may need to edit the path, depending on where you put the files.\n","a4data = pd.read_csv('raisins.csv')\n","\n","X = scale(a4data.drop(columns='Class'))\n","Y = 1.0*(a4data.Class == 'Besni').to_numpy()\n","\n","np.random.seed(0)\n","shuffle = np.random.permutation(len(Y))\n","X = X[shuffle]\n","Y = Y[shuffle]\n","\n","Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, random_state=0, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"727929a2","source_hash":null,"execution_start":1708942044172,"execution_millis":115,"deepnote_to_be_reexecuted":false,"cell_id":"a3068f0521f3494b876c993f9e35feb8","deepnote_cell_type":"code","outputId":"6fc19269-ba05-4112-8ecf-c5804f82a1ef"},"source":["Xtrain.shape, Ytrain.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"((720, 7), (720,))"},"metadata":{}}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1708942044179,"execution_millis":126,"deepnote_to_be_reexecuted":false,"cell_id":"ab0a9a47c91b4a6eba5f5da039c4467e","deepnote_cell_type":"code","id":"ar0cURcFXjTo"},"source":["#Initilizing the new type of nodes needed\n","\n","# Backward gradient calculations as per:\n","# Tanh: 1 - (tanh(x))^2\n","# Sigmoid: x(1-x)\n","# BinaryCrossEntropy:\n","\n","class TanhNode(Node):\n","    def __init__(self, input_tensor):\n","        self.input_tensor = input_tensor\n","\n","    def backward(self, grad_output):\n","        if self.input_tensor.requires_grad:\n","            self.input_tensor.backward((1 - np.square(np.tanh(self.input_tensor.data))) * grad_output)\n","\n","class SigmoidNode(Node):\n","    def __init__(self, input_tensor):\n","        super().__init__()\n","        self.input_tensor = input_tensor\n","\n","    def backward(self, grad_output):\n","        if self.input_tensor.requires_grad:\n","            self.input_tensor.backward((self.input_tensor.data * (1 - self.input_tensor.data)) * grad_output)\n","\n","\n","class BinaryCrossEntropyNode(Node):\n","    def __init__(self, input_tensor):\n","        super().__init__()\n","        self.input_tensor = input_tensor\n","\n","    def backward(self, grad_output):\n","        if self.input_tensor.requires_grad:\n","            # Compute the gradient of binary cross-entropy loss with respect to input_tensor\n","            grad_loss = -((self.input_tensor.data[0].data / self.input_tensor.data[1].data[0][0]) - ((1 - self.input_tensor.data[0].data) / (1 - self.input_tensor.data[1].data[0][0])))\n","            # Backpropagate the gradient\n","            self.input_tensor.backward(grad_output * grad_loss)\n","\n","    def forward(self):\n","       # https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#BCELoss\n","       # def forward(self, input: Tensor, target: Tensor) -> Tensor:\n","       # return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n","\n","       # https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/\n","       ytrue = self.input_tensor.data[0].data\n","       ypred = self.input_tensor.data[1].data[0][0]\n","       return -(ytrue*np.log(ypred) + (1-ytrue)*np.log(1-ypred))\n","\n","class BinaryCrossEntropyNode2(Node):\n","    def __init__(self, left, right):\n","        super().__init__()\n","        self.left = left #y_pred\n","        self.right = right #y\n","\n","    def backward(self, grad_output):\n","        if self.left.requires_grad or self.right.requires_grad:\n","            self.left.backward(grad_output * (0-self.right.data) * ((0-1)/(1+np.exp(0-self.left.data) * (1/(1+np.exp(0-self.left.data))))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1708942044191,"execution_millis":114,"deepnote_to_be_reexecuted":false,"cell_id":"e471c977622e43238c3e66a3aba6987e","deepnote_cell_type":"code","id":"-8kU6_U9XjTp"},"source":["#Defining new functions needed\n","\n","def tanh(input_tensor):\n","    output_tensor = Tensor(np.tanh(input_tensor.data), requires_grad=input_tensor.requires_grad)\n","    if input_tensor.requires_grad:\n","        output_tensor.grad_fn = TanhNode(input_tensor)\n","    return output_tensor\n","\n","def sigmoid(input_tensor):\n","    output_tensor = Tensor(1 / (1 + np.exp(-input_tensor.data)), requires_grad=input_tensor.requires_grad)\n","    if input_tensor.requires_grad:\n","        output_tensor.grad_fn = SigmoidNode(input_tensor)\n","    return output_tensor\n","\n","def binary_cross_entropy(input_tensor):\n","    loss_tensor = Tensor(BinaryCrossEntropyNode(input_tensor).forward(), requires_grad=y_pred.requires_grad)\n","\n","    if y_pred.requires_grad:\n","        loss_tensor.grad_fn = BinaryCrossEntropyNode(input_tensor)\n","    return loss_tensor\n","\n","#Remove before we hand in if this is useless\n","def binary_cross_entropy2(y_pred, y_true):\n","    # Assuming y_pred and y_true are Tensors with requires_grad set appropriately for y_pred\n","    # Compute the binary cross entropy loss\n","    bce_loss_value = -(y_true.data * np.log(y_pred.data) + (1 - y_true.data) * np.log(1 - y_pred.data))\n","    loss_tensor = Tensor(np.array([bce_loss_value]), requires_grad=y_pred.requires_grad)\n","\n","    if y_pred.requires_grad:\n","        loss_tensor.grad_fn = BinaryCrossEntropyNode2(y_pred, y_true)\n","\n","    return loss_tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1708942044192,"execution_millis":267,"deepnote_to_be_reexecuted":false,"cell_id":"d5229aa4882143cdbdcbd64ec5a7ab48","deepnote_cell_type":"code","id":"wsVBFnYhXjTq"},"source":["import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1708942077481,"execution_millis":745,"deepnote_to_be_reexecuted":false,"cell_id":"3edf7d9b6d0643f39d2544ad39fdbefb","deepnote_cell_type":"code","id":"M6RXSZdoXjTq","outputId":"a0916417-f94c-4652-9a3d-67bf271574ee"},"source":["#Initilizing the model structure to have one hidden layer and binary classification\n","\n","h_size = 36 #Randomly choosing a hidden layer size, this should be adjusted/tested\n","o_size = 1 #Output size = 1 since task is binary classification\n","\n","#Initilizing the first hidden layer\n","w1 = tensor(np.random.normal(0,1, (Xtrain.shape[1], h_size)), requires_grad=True)\n","b1 = tensor(np.zeros((1,h_size)), requires_grad=True)\n","\n","#Initilizing the output layer\n","w2 = tensor(np.random.normal(0,1,(h_size, o_size)), requires_grad=True)\n","b2 = tensor(np.zeros((1,o_size)), requires_grad=True)\n","\n","\n","#Training the model\n","epochs = 10 #Number of epochs to be adjusted/tested\n","eta = 1e-2\n","opt = SGD([w1,b1,w2,b2], lr=eta)\n","\n","plot_data = []\n","\n","for i in range(epochs):\n","\n","    sum_err = 0\n","\n","    for row in range(Xtrain.shape[0]):\n","        x = tensor(Xtrain[[row], :], requires_grad=False)\n","        y = tensor(Ytrain[[row]], requires_grad=False)\n","\n","        # Hidden layer\n","        z1 = x @ w1 + b1\n","        h1 = tanh(z1)\n","\n","        #Output layer\n","        z2 = h1 @ w2 + b2\n","        y_pred = sigmoid(z2)\n","\n","        # Create a tensor with y_true and y_pred\n","        loss_input = tensor(np.array((y, y_pred)), requires_grad=True)\n","\n","        # Binary cross entropy loss\n","        err = binary_cross_entropy(loss_input)\n","        #err = binary_cross_entropy2(y_pred, y) #Remove before hand in if not used\n","\n","        # Backward and update.\n","        opt.zero_grad() # Resets the gradients of all optimized\n","        err.backward() # Computes the gradient of current tensor wrt graph leaves\n","        opt.step() # Performs a single optimization step (parameter update)\n","\n","        # For statistics.\n","        sum_err += err.item()\n","\n","    log_loss = sum_err / Xtrain.shape[0]\n","    print(f'Epoch {i+1}: Log loss =', log_loss)\n","    plot_data.append(log_loss)\n","\n","plt.plot(plot_data)\n","plt.title('Log loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Log loss')\n","plt.show()"],"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1: Log loss = 0.8479121498632477\nEpoch 2: Log loss = 0.8479121498632477\nEpoch 3: Log loss = 0.8479121498632477\nEpoch 4: Log loss = 0.8479121498632477\nEpoch 5: Log loss = 0.8479121498632477\nEpoch 6: Log loss = 0.8479121498632477\nEpoch 7: Log loss = 0.8479121498632477\nEpoch 8: Log loss = 0.8479121498632477\nEpoch 9: Log loss = 0.8479121498632477\nEpoch 10: Log loss = 0.8479121498632477\n","output_type":"stream"},{"data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{"image/png":{"width":576,"height":455}},"output_type":"display_data"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"e4695799efed4579b7b67f303e17c9b6","deepnote_cell_type":"text-cell-p","id":"DdcLE2RkXjTr"},"source":["Something is clearly wrong in our implementation here, but we are not sure what. Our implemetnation of BCE seems to always return a gradient of 0, hence why the log loss does not update. Running with our other implementation, however, gives a matrix multiplication error in the backward() function in MultiplicationNode, so we rewrote it like the below:"]},{"cell_type":"markdown","metadata":{"cell_id":"96428112e2ce4775aa173b0db25542e2","deepnote_cell_type":"markdown","id":"-oYg_gHTXjTs"},"source":[">MultiplicationNode class for Task 6\n","    class MultiplicationNode(Node):\n","        def __init__(self, left, right):\n","            self.left = left\n","            self.right = right\n","    \n","        def backward(self, grad_output):\n","        if self.left.requires_grad:\n","            if len(self.right.data.shape) > 1:  # Matrix\n","                grad_left = np.dot(grad_output, self.right.data.T)\n","            else:\n","                grad_left = (self.right.data * grad_output)\n","            self.left.backward(grad_left)\n","        if self.right.requires_grad:\n","            if len(self.left.data.shape) > 1:\n","                grad_right = np.dot(self.left.data.T, grad_output)\n","            else:\n","                grad_right = (self.left.data * grad_output)\n","            self.right.backward(grad_right)"]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"f6b0160abde84f77848df11e9f8c1d51","deepnote_cell_type":"text-cell-p","id":"lx_rIIICXjTt"},"source":["This yields working code in Task 6, but results in errors in Task 4 and 5. With this working Task 6, the next step would be to evaluate the model on the testing data to see the accuracy.\n","\n","However, due to the approaching deadline, we are not able to fix these issues and so we will submit it as is (with the mandatory tasks 1-5 working)."]},{"cell_type":"markdown","source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1b094071-690d-44cb-8cd4-a16b2af2a809' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"],"metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","id":"qaZlt5aFXjTt"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-02-26T10:30:45.296Z"},"deepnote_notebook_id":"0e371b807f7044f3985e38a0d631dd8f","deepnote_execution_queue":[],"colab":{"provenance":[]}}}